{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import io","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-30T16:48:56.256481Z","iopub.execute_input":"2024-06-30T16:48:56.256855Z","iopub.status.idle":"2024-06-30T16:48:56.269490Z","shell.execute_reply.started":"2024-06-30T16:48:56.256826Z","shell.execute_reply":"2024-06-30T16:48:56.268467Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"**Q1**","metadata":{}},{"cell_type":"code","source":"! pip install gdown","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:49:15.162051Z","iopub.execute_input":"2024-06-30T16:49:15.162774Z","iopub.status.idle":"2024-06-30T16:49:28.697815Z","shell.execute_reply.started":"2024-06-30T16:49:15.162739Z","shell.execute_reply":"2024-06-30T16:49:28.696846Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.2.2)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport torchvision\nimport torch.nn as nn # use nn functions like sigmoid, ReLu, softmax\nimport torchvision.transforms as transforms # use for transformerin on data\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset,DataLoader\nimport torchvision.models as models\nimport os\nfrom PIL import Image\nimport io\nimport zipfile\nfrom torch.utils.data import DataLoader, random_split\nimport torch.nn.functional as F\nimport time\nfrom scipy.spatial.distance import pdist, squareform\nfrom sklearn.decomposition import PCA\nimport json\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:49:31.631875Z","iopub.execute_input":"2024-06-30T16:49:31.632759Z","iopub.status.idle":"2024-06-30T16:49:37.394112Z","shell.execute_reply.started":"2024-06-30T16:49:31.632721Z","shell.execute_reply":"2024-06-30T16:49:37.393340Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"! gdown 1hAfkobN0YBk2D2fqhhRht2yKmF1nNwWz","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:49:41.227306Z","iopub.execute_input":"2024-06-30T16:49:41.228350Z","iopub.status.idle":"2024-06-30T16:49:43.480992Z","shell.execute_reply.started":"2024-06-30T16:49:41.228316Z","shell.execute_reply":"2024-06-30T16:49:43.479831Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1hAfkobN0YBk2D2fqhhRht2yKmF1nNwWz\nTo: /kaggle/working/kaggle.json\n100%|█████████████████████████████████████████| 64.0/64.0 [00:00<00:00, 340kB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!mkdir -p ~/.kaggle\n!mv kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:49:47.532051Z","iopub.execute_input":"2024-06-30T16:49:47.532468Z","iopub.status.idle":"2024-06-30T16:49:50.390528Z","shell.execute_reply.started":"2024-06-30T16:49:47.532432Z","shell.execute_reply":"2024-06-30T16:49:50.389390Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!kaggle datasets download -d danielbacioiu/tig-aluminium-5083","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:49:54.246984Z","iopub.execute_input":"2024-06-30T16:49:54.247818Z","iopub.status.idle":"2024-06-30T16:50:41.555966Z","shell.execute_reply.started":"2024-06-30T16:49:54.247784Z","shell.execute_reply":"2024-06-30T16:50:41.555003Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Dataset URL: https://www.kaggle.com/datasets/danielbacioiu/tig-aluminium-5083\nLicense(s): CC-BY-SA-4.0\nDownloading tig-aluminium-5083.zip to /kaggle/working\n100%|██████████████████████████████████████▉| 11.2G/11.2G [00:45<00:00, 280MB/s]\n100%|███████████████████████████████████████| 11.2G/11.2G [00:45<00:00, 263MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"! pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:55:47.025592Z","iopub.execute_input":"2024-06-30T16:55:47.026407Z","iopub.status.idle":"2024-06-30T16:56:00.149323Z","shell.execute_reply.started":"2024-06-30T16:55:47.026379Z","shell.execute_reply":"2024-06-30T16:56:00.148137Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom PIL import Image\nimport zipfile\nimport json\nimport io\nimport os\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# Check GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Download and set up Kaggle credentials (if needed)\n# Note: Adjust your download and setup steps here accordingly\n! gdown 1hAfkobN0YBk2D2fqhhRht2yKmF1nNwWz\n! mkdir -p ~/.kaggle\n! mv kaggle.json ~/.kaggle/\n! chmod 600 ~/.kaggle/kaggle.json\n\n# Download dataset (assuming you've adjusted the download commands)\n! kaggle datasets download -d danielbacioiu/tig-aluminium-5083\n\n# Define transformations for image preprocessing\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n])\n\nclass SMDataset(Dataset):\n    def __init__(self, zip_file_path, folder_name, json_file_path, transform=None):\n        self.zip_file = zipfile.ZipFile(zip_file_path, 'r')\n        self.folder_name = folder_name.rstrip('/') + '/'\n        self.file_list = [name for name in self.zip_file.namelist() if name.startswith(self.folder_name) and name.endswith('.png')]\n        self.transform = transform\n\n        # Load labels from JSON file\n        with self.zip_file.open(json_file_path) as json_file:\n            self.labels_dict = json.load(json_file)\n\n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx):\n        try:\n            with self.zip_file.open(self.file_list[idx]) as file:\n                image = Image.open(io.BytesIO(file.read())).convert('RGB')\n                if self.transform:\n                    image = self.transform(image)\n\n            # Extract the relevant part of the filename for lookup\n            file_name = self.file_list[idx].replace(self.folder_name, '')\n\n            # Get the label for the current file\n            label = self.labels_dict.get(file_name, -1)\n\n            return image, label\n        except Exception as e:\n            print(f\"Error loading item {idx}: {e}\")\n            return None, None\n\n# Define paths for train and test JSON files\ntrain_json_path = 'al5083/train/train.json'\ntest_json_path = 'al5083/test/test.json'\n\n# Create dataset instances\ntotal_train_dataset = SMDataset(\n    zip_file_path='/kaggle/working/tig-aluminium-5083.zip',\n    folder_name='al5083/train',\n    json_file_path=train_json_path,\n    transform=transform\n)\n\ntotal_test_dataset = SMDataset(\n    zip_file_path='/kaggle/working/tig-aluminium-5083.zip',\n    folder_name='al5083/test',\n    json_file_path=test_json_path,\n    transform=transform\n)\n\n# Split datasets into train, validation, and additional 90% train datasets\ntotal_train_size = len(total_train_dataset)\ntotal_test_size = len(total_test_dataset)\n\ntrain_size = int(0.8 * total_train_size)\nvalid_size = int(0.5 * total_train_size)\ntrain90_size = total_train_size - train_size - valid_size\n\ntest_size = int(0.1 * total_test_size)\ntest90_size = total_test_size - test_size\n\ntrain_dataset, valid_dataset, train90_dataset = random_split(total_train_dataset, [train_size, valid_size, train90_size])\ntest_dataset, test90_dataset = random_split(total_test_dataset, [test_size, test90_size])\n\n# Create DataLoaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=True)\n\n# Define MyModel in PyTorch\nclass MyModel(nn.Module):\n    def __init__(self, num_classes=6, l2_reg=0.01):\n        super(MyModel, self).__init__()\n        self.conv1 = nn.Conv2d(3, 256, kernel_size=3, padding=1)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout1 = nn.Dropout(p=0.2)\n\n        self.conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout2 = nn.Dropout(p=0.2)\n\n        self.conv3 = nn.Conv2d(128, 64, kernel_size=3)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.flatten = nn.Flatten()\n\n        self.fc1 = nn.Linear(64 * 31 * 31, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool1(x)\n        x = self.dropout1(x)\n\n        x = F.relu(self.conv2(x))\n        x = self.pool2(x)\n        x = self.dropout2(x)\n\n        x = F.relu(self.conv3(x))\n        x = self.pool3(x)\n\n        x = self.flatten(x)\n\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n\n        return x\n\n# Initialize model and move to appropriate device\nmodel = MyModel().to(device)\n\n# Print model summary\nfrom torchsummary import summary\nsummary(model, input_size=(3, 256, 256))\n\n# Count total number of parameters in the model\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total number of parameters: {total_params}\")\n\n# Define loss function and optimizer\nlossfn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n\n# Training loop\nnum_epochs = 20\nfor epoch in range(num_epochs):\n    # Training phase\n    model.train()\n    train_loss = 0.0\n    train_batches = len(train_dataloader)\n    with tqdm(total=train_batches, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as pbar:\n        for i, (images, labels) in enumerate(train_dataloader):\n            images = images.to(device)\n            labels = labels.to(device)\n\n            # Forward pass\n            outputs = model(images)\n            loss = lossfn(outputs, labels)\n\n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n\n            if i % 16 == 0:\n                pbar.set_postfix({'loss': train_loss / (i + 1)})\n                pbar.update(16)\n\n    # Validation phase\n    model.eval()\n    correct = 0\n    total = 0\n    valid_batches = len(valid_dataloader)\n    with tqdm(total=valid_batches, desc=f'Validation {epoch+1}/{num_epochs}', unit='batch') as pbar:\n        with torch.no_grad():\n            for images, labels in valid_dataloader:\n                images = images.to(device)\n                labels = labels.to(device)\n\n                # Forward pass\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n                pbar.update(1)\n\n        accuracy = 100 * correct / total\n        print(f'Accuracy of the network on the validation images: {accuracy:.2f} %')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:57:26.268994Z","iopub.execute_input":"2024-06-30T16:57:26.269353Z","iopub.status.idle":"2024-06-30T19:57:04.136384Z","shell.execute_reply.started":"2024-06-30T16:57:26.269328Z","shell.execute_reply":"2024-06-30T19:57:04.135193Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1hAfkobN0YBk2D2fqhhRht2yKmF1nNwWz\nTo: /kaggle/working/kaggle.json\n100%|█████████████████████████████████████████| 64.0/64.0 [00:00<00:00, 380kB/s]\nDataset URL: https://www.kaggle.com/datasets/danielbacioiu/tig-aluminium-5083\nLicense(s): CC-BY-SA-4.0\ntig-aluminium-5083.zip: Skipping, found more recently modified local copy (use --force to force download)\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1        [-1, 256, 256, 256]           7,168\n         MaxPool2d-2        [-1, 256, 128, 128]               0\n           Dropout-3        [-1, 256, 128, 128]               0\n            Conv2d-4        [-1, 128, 128, 128]         295,040\n         MaxPool2d-5          [-1, 128, 64, 64]               0\n           Dropout-6          [-1, 128, 64, 64]               0\n            Conv2d-7           [-1, 64, 62, 62]          73,792\n         MaxPool2d-8           [-1, 64, 31, 31]               0\n           Flatten-9                [-1, 61504]               0\n           Linear-10                  [-1, 256]      15,745,280\n           Linear-11                  [-1, 128]          32,896\n           Linear-12                    [-1, 6]             774\n================================================================\nTotal params: 16,154,950\nTrainable params: 16,154,950\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.75\nForward/backward pass size (MB): 218.82\nParams size (MB): 61.63\nEstimated Total Size (MB): 281.19\n----------------------------------------------------------------\nTotal number of parameters: 16154950\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/20: 1344batch [07:26,  3.01batch/s, loss=0.669]                       \nValidation 1/20: 100%|██████████| 5334/5334 [01:32<00:00, 57.46batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 86.50 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 1344batch [07:26,  3.01batch/s, loss=0.195]                       \nValidation 2/20: 100%|██████████| 5334/5334 [01:32<00:00, 57.42batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 90.57 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 1344batch [07:26,  3.01batch/s, loss=0.112]                       \nValidation 3/20: 100%|██████████| 5334/5334 [01:32<00:00, 57.36batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 97.62 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 1344batch [07:26,  3.01batch/s, loss=0.0757]                       \nValidation 4/20: 100%|██████████| 5334/5334 [01:33<00:00, 57.26batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 94.75 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 1344batch [07:27,  3.00batch/s, loss=0.0589]                       \nValidation 5/20: 100%|██████████| 5334/5334 [01:32<00:00, 57.52batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 98.86 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/20: 1344batch [07:25,  3.01batch/s, loss=0.0526]                       \nValidation 6/20: 100%|██████████| 5334/5334 [01:33<00:00, 57.20batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 98.69 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/20: 1344batch [07:28,  3.00batch/s, loss=0.0453]                       \nValidation 7/20: 100%|██████████| 5334/5334 [01:32<00:00, 57.48batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 99.27 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/20: 1344batch [07:25,  3.02batch/s, loss=0.0373]                       \nValidation 8/20: 100%|██████████| 5334/5334 [01:32<00:00, 57.44batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 98.52 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/20: 1344batch [07:25,  3.02batch/s, loss=0.0321]                       \nValidation 9/20: 100%|██████████| 5334/5334 [01:33<00:00, 57.16batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 99.23 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/20: 1344batch [07:24,  3.02batch/s, loss=0.0304]                       \nValidation 10/20: 100%|██████████| 5334/5334 [01:33<00:00, 57.34batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 99.33 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/20: 1344batch [07:27,  3.01batch/s, loss=0.0248]                       \nValidation 11/20: 100%|██████████| 5334/5334 [01:33<00:00, 57.34batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 99.14 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/20: 1344batch [07:26,  3.01batch/s, loss=0.0249]                       \nValidation 12/20: 100%|██████████| 5334/5334 [01:32<00:00, 57.42batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 97.54 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/20: 1344batch [07:24,  3.02batch/s, loss=0.0192]                       \nValidation 13/20: 100%|██████████| 5334/5334 [01:32<00:00, 57.55batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 98.74 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/20: 1344batch [07:24,  3.02batch/s, loss=0.0189]                       \nValidation 14/20: 100%|██████████| 5334/5334 [01:33<00:00, 57.20batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 98.78 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/20: 1344batch [07:24,  3.02batch/s, loss=0.0185]                       \nValidation 15/20: 100%|██████████| 5334/5334 [01:32<00:00, 57.95batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 99.40 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/20: 1344batch [07:24,  3.02batch/s, loss=0.016]                        \nValidation 16/20: 100%|██████████| 5334/5334 [01:32<00:00, 57.93batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 99.68 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/20: 1344batch [07:24,  3.02batch/s, loss=0.0163]                       \nValidation 17/20: 100%|██████████| 5334/5334 [01:32<00:00, 57.67batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 99.31 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/20: 1344batch [07:24,  3.03batch/s, loss=0.0138]                       \nValidation 18/20: 100%|██████████| 5334/5334 [01:32<00:00, 57.84batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 99.64 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/20: 1344batch [07:25,  3.02batch/s, loss=0.0136]                       \nValidation 19/20: 100%|██████████| 5334/5334 [01:32<00:00, 57.94batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 99.25 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/20: 1344batch [07:23,  3.03batch/s, loss=0.0148]                       \nValidation 20/20: 100%|██████████| 5334/5334 [01:32<00:00, 57.71batch/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the validation images: 99.76 %\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"correct = 0\ntotal = 0\nfor images,labels in test_dataloader:\n  images = images.to(device)\n  labels = labels.to(device)\n  outputs = model(images)\n  prediction = torch.argmax(outputs, dim = 1)\n  correct += (prediction == labels).sum().item()\n  total += labels.size(0)\n\nprint(correct/total * 100)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:58:47.930995Z","iopub.execute_input":"2024-06-30T19:58:47.931898Z","iopub.status.idle":"2024-06-30T19:58:58.900632Z","shell.execute_reply.started":"2024-06-30T19:58:47.931864Z","shell.execute_reply":"2024-06-30T19:58:58.899733Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"41.33738601823708\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\nimport torch\n\ndef calculate_fps(model, dataloader, device, num_batches=10):\n    model.eval()  # Set the model to evaluation mode\n    \n    total_time = 0.0\n    total_images = 0\n    \n    with torch.no_grad():\n        for i, (images, labels) in enumerate(dataloader):\n            if i >= num_batches:\n                break\n            \n            images = images.to(device)\n            total_images += images.size(0)\n            \n            start_time = time.time()\n            outputs = model(images)\n            end_time = time.time()\n            \n            total_time += (end_time - start_time)\n    \n    fps = total_images / total_time\n    return fps\n\nfps = calculate_fps(model, valid_dataloader, device, num_batches=10)\nprint(f'FPS: {fps:.2f}')  # Corrected indentation here","metadata":{"execution":{"iopub.status.busy":"2024-06-30T20:00:49.944380Z","iopub.execute_input":"2024-06-30T20:00:49.945120Z","iopub.status.idle":"2024-06-30T20:00:50.122904Z","shell.execute_reply.started":"2024-06-30T20:00:49.945087Z","shell.execute_reply":"2024-06-30T20:00:50.121993Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"FPS: 1530.99\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Q2**","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Example image (5x5)\nimage = np.array([[0.51, 0.90, 0.88, 0.84, 0.05],\n                  [0.40, 0.62, 0.22, 0.59, 0.10],\n                  [0.11, 0.20, 0.74, 0.33, 0.14],\n                  [0.47, 0.01, 0.85, 0.70, 0.09],\n                  [0.76, 0.19, 0.72, 0.17, 0.57]])\n\n# Corrected kernel (2x2)\nkernel_1 = np.array([[-0.13, 0.15], [-0.51, 0.62]])\n\n# Convolution function\ndef convolve(image, kernel, stride=1, padding=0):\n    kernel_height, kernel_width = kernel.shape\n    image_height, image_width = image.shape\n\n    # Apply padding\n    if padding > 0:\n        image = np.pad(image, ((padding, padding), (padding, padding)), mode='constant')\n\n    # Determine output dimensions\n    out_height = (image_height - kernel_height) // stride + 1\n    out_width = (image_width - kernel_width) // stride + 1\n\n    # Initialize output\n    output = np.zeros((out_height, out_width))\n\n    # Perform convolution\n    for y in range(out_height):\n        for x in range(out_width):\n            output[y, x] = np.sum(image[y*stride:y*stride+kernel_height, x*stride:x*stride+kernel_width] * kernel)\n\n    return output\n\n# First convolution output\nconv_output_1 = convolve(image, kernel_1)\nprint(\"First Convolution Output:\\n\", conv_output_1)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:54:19.744307Z","iopub.execute_input":"2024-06-30T21:54:19.744711Z","iopub.status.idle":"2024-06-30T21:54:19.757313Z","shell.execute_reply.started":"2024-06-30T21:54:19.744681Z","shell.execute_reply":"2024-06-30T21:54:19.756438Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"First Convolution Output:\n [[ 0.2491 -0.1648  0.2652 -0.3406]\n [ 0.1089  0.3092 -0.1129 -0.1432]\n [-0.2178  0.6069 -0.0462 -0.3231]\n [-0.3294  0.4757 -0.2673  0.1892]]\n","output_type":"stream"}]},{"cell_type":"code","source":"def max_pooling(feature_map, size=2, stride=2):\n    pooled_height = (feature_map.shape[0] - size) // stride + 1\n    pooled_width = (feature_map.shape[1] - size) // stride + 1\n\n    pooled_output = np.zeros((pooled_height, pooled_width))\n\n    for y in range(pooled_height):\n        for x in range(pooled_width):\n            pooled_output[y, x] = np.max(feature_map[y*stride:y*stride+size, x*stride:x*stride+size])\n\n    return pooled_output\n\n# Max-pooling output\npooled_output = max_pooling(conv_output_1)\nprint(\"Max-Pooling Output:\\n\", pooled_output)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:54:46.769581Z","iopub.execute_input":"2024-06-30T21:54:46.770168Z","iopub.status.idle":"2024-06-30T21:54:46.777274Z","shell.execute_reply.started":"2024-06-30T21:54:46.770134Z","shell.execute_reply":"2024-06-30T21:54:46.776204Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Max-Pooling Output:\n [[0.3092 0.2652]\n [0.6069 0.1892]]\n","output_type":"stream"}]},{"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Sigmoid output\nsigmoid_output = sigmoid(pooled_output)\nprint(\"Sigmoid Output:\\n\", sigmoid_output)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:55:10.822878Z","iopub.execute_input":"2024-06-30T21:55:10.823496Z","iopub.status.idle":"2024-06-30T21:55:10.829824Z","shell.execute_reply.started":"2024-06-30T21:55:10.823466Z","shell.execute_reply":"2024-06-30T21:55:10.828854Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Sigmoid Output:\n [[0.57668998 0.56591413]\n [0.64723333 0.5471594 ]]\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\n# Given image and kernel\nimage = np.array([[0.51, 0.90, 0.88, 0.84, 0.05],\n                  [0.40, 0.62, 0.22, 0.59, 0.10],\n                  [0.11, 0.20, 0.74, 0.33, 0.14],\n                  [0.47, 0.01, 0.85, 0.70, 0.09],\n                  [0.76, 0.19, 0.72, 0.17, 0.57]])\n\nkernel = np.array([[-0.13, 0.15], [-0.51, 0.62]])\n\n# Assuming output size and initial weights and biases based on your description\noutput_size = 2\nweights_fc = np.array([[0.61, 0.82, 0.96, -1.0],\n                       [0.02, -0.5, 0.23, 0.17]])\nbiases_fc = np.array([0.25, -0.15])\n\nlearning_rate = 0.5\n\nclass NeuralNetwork:\n    def __init__(self, kernel, weights_fc, biases_fc):\n        # Initialize kernel and fully connected layer weights and biases\n        self.kernel = kernel\n        self.weights_fc = weights_fc\n        self.biases_fc = biases_fc\n    \n    def convolve(self, image):\n        # Perform convolution\n        image_height, image_width = image.shape\n        kernel_height, kernel_width = self.kernel.shape\n        \n        convolved_height = image_height - kernel_height + 1\n        convolved_width = image_width - kernel_width + 1\n        \n        convolved = np.zeros((convolved_height, convolved_width))\n        \n        for i in range(convolved_height):\n            for j in range(convolved_width):\n                patch = image[i:i+kernel_height, j:j+kernel_width]\n                convolved[i, j] = np.sum(patch * self.kernel)\n        \n        return convolved\n    \n    def max_pooling(self, convolved):\n        # Perform max pooling (assuming 2x2 pooling with stride 2)\n        pooled_height = convolved.shape[0] // 2\n        pooled_width = convolved.shape[1] // 2\n        \n        pooled = np.zeros((pooled_height, pooled_width))\n        \n        for i in range(pooled_height):\n            for j in range(pooled_width):\n                patch = convolved[2*i:2*i+2, 2*j:2*j+2]\n                pooled[i, j] = np.max(patch)\n        \n        return pooled\n    \n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n    \n    def flatten(self, pooled):\n        return pooled.flatten()\n    \n    def forward_fc(self, flattened):\n        # Forward pass through fully connected layer\n        z = np.dot(flattened, self.weights_fc.T) + self.biases_fc\n        return self.sigmoid(z)\n    \n    def backward_fc(self, flattened, y_pred):\n        # Backward pass through fully connected layer\n        grad_z = (y_pred - 1) * y_pred * (1 - y_pred)  # Example derivative of sigmoid with respect to z\n        \n        delta_weights_fc = np.outer(flattened, grad_z)\n        delta_biases_fc = grad_z\n        \n        return delta_weights_fc, delta_biases_fc\n\n    def update_params(self, delta_weights_fc, delta_biases_fc, learning_rate):\n        # Update fully connected layer weights and biases\n        self.weights_fc += learning_rate * delta_weights_fc.T  # Transpose delta_weights_fc\n        self.biases_fc += learning_rate * delta_biases_fc\n\n    def train(self, image):\n        # Forward pass\n        convolved = self.convolve(image)\n        pooled = self.max_pooling(convolved)\n        flattened = self.flatten(pooled)\n        y_pred = self.forward_fc(flattened)\n        \n        # Backward pass\n        delta_weights_fc, delta_biases_fc = self.backward_fc(flattened, y_pred)\n        \n        # Update weights and biases\n        self.update_params(delta_weights_fc, delta_biases_fc, learning_rate)\n        \n        return y_pred, delta_weights_fc, delta_biases_fc\n\n# Create an instance of the NeuralNetwork class\nNN = NeuralNetwork(kernel, weights_fc, biases_fc)\n\n# Train the network (for demonstration purposes, we're not iterating over batches here)\ny_pred, delta_weights_fc, delta_biases_fc = NN.train(image)\n\n# Print the changes in parameters\nprint(\"Changes in Fully Connected Layer Weights:\")\nprint(delta_weights_fc)\nprint(\"\\nChanges in Fully Connected Layer Biases:\")\nprint(delta_biases_fc)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:59:38.664330Z","iopub.execute_input":"2024-06-30T21:59:38.665005Z","iopub.status.idle":"2024-06-30T21:59:38.685741Z","shell.execute_reply.started":"2024-06-30T21:59:38.664978Z","shell.execute_reply":"2024-06-30T21:59:38.684719Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Changes in Fully Connected Layer Weights:\n[[-0.01540087 -0.04055962]\n [-0.01320928 -0.03478788]\n [-0.03022894 -0.07961073]\n [-0.00942382 -0.0248185 ]]\n\nChanges in Fully Connected Layer Biases:\n[-0.04980876 -0.13117602]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"fold/unfold","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Given image and kernel\nimage = np.array([[0.51, 0.90, 0.88, 0.84, 0.05],\n                  [0.40, 0.62, 0.22, 0.59, 0.10],\n                  [0.11, 0.20, 0.74, 0.33, 0.14],\n                  [0.47, 0.01, 0.85, 0.70, 0.09],\n                  [0.76, 0.19, 0.72, 0.17, 0.57]])\n\nkernel = np.array([[-0.13, 0.15], [-0.51, 0.62]])\n\nclass NeuralNetwork:\n    def __init__(self, kernel):\n        self.kernel = kernel\n    \n    def unfold(self, image, kernel_size):\n        image_height, image_width = image.shape\n        kernel_height, kernel_width = kernel_size\n        \n        patches = []\n        for i in range(image_height - kernel_height + 1):\n            for j in range(image_width - kernel_width + 1):\n                patch = image[i:i+kernel_height, j:j+kernel_width]\n                patches.append(patch.flatten())\n                \n        return np.array(patches)\n    \n    def fold(self, patches, output_shape):\n        output_height, output_width = output_shape\n        return patches.reshape(output_height, output_width)\n    \n    def convolve(self, image):\n        kernel_height, kernel_width = self.kernel.shape\n        \n        patches = self.unfold(image, self.kernel.shape)\n        convolved = np.dot(patches, self.kernel.flatten())\n        \n        convolved_height = image.shape[0] - kernel_height + 1\n        convolved_width = image.shape[1] - kernel_width + 1\n        \n        convolved = self.fold(convolved, (convolved_height, convolved_width))\n        \n        return convolved\n\n# Create an instance of the NeuralNetwork class\nNN = NeuralNetwork(kernel)\n\n# Perform convolution\nconvolved = NN.convolve(image)\n\nprint(\"Convolved Image:\")\nprint(convolved)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T22:09:10.218375Z","iopub.execute_input":"2024-06-30T22:09:10.219220Z","iopub.status.idle":"2024-06-30T22:09:10.232751Z","shell.execute_reply.started":"2024-06-30T22:09:10.219180Z","shell.execute_reply":"2024-06-30T22:09:10.231856Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Convolved Image:\n[[ 0.2491 -0.1648  0.2652 -0.3406]\n [ 0.1089  0.3092 -0.1129 -0.1432]\n [-0.2178  0.6069 -0.0462 -0.3231]\n [-0.3294  0.4757 -0.2673  0.1892]]\n","output_type":"stream"}]}]}